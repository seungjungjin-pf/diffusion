{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "56293b0f-0e94-4174-86f4-ede3d50aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84364b3f-9628-4881-a33d-cc29ab61f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from datasets import FillDataset\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "\n",
    "\n",
    "from diffusers import SD3Transformer2DModel, AutoencoderKL\n",
    "\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "from text_embed import encode_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf5d770-f1e5-4e85-aa45-7468b55c3559",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 14:41:04.805172: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-25 14:41:04.805223: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-25 14:41:04.806117: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-25 14:41:04.811682: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-25 14:41:05.629864: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ecc87feb06d466ab07ea3a0b29915f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading pipeline components...:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66ad89438ce4426a9b7b8ba73307255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from diffusers import StableDiffusion3Pipeline\n",
    "\n",
    "pipe = StableDiffusion3Pipeline.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\", torch_dtype=torch.float16)\n",
    "pipe = pipe.to(\"cuda\")\n",
    "\n",
    "image = pipe(\n",
    "    \"Cyan circle with brown background\",\n",
    "    negative_prompt=\"\",\n",
    "    num_inference_steps=28,\n",
    "    guidance_scale=7.0,\n",
    ").images[0]\n",
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5032f395-31c3-41b4-beb6-225308f7d237",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FillDataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mFillDataset\u001b[49m()\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(dataset))\n\u001b[1;32m      4\u001b[0m item \u001b[38;5;241m=\u001b[39m dataset[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'FillDataset' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = FillDataset()\n",
    "print(len(dataset))\n",
    "\n",
    "item = dataset[0]\n",
    "jpg = item['jpg']\n",
    "txt = item['txt']\n",
    "hint = item['hint']\n",
    "print(txt)\n",
    "print(jpg.shape)\n",
    "print(hint.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "153c7b7b-b128-4093-abdf-91a1cdaed6c3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jpg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mjpg\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jpg' is not defined"
     ]
    }
   ],
   "source": [
    "jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f2d8af-4220-4848-a978-521719a2fe86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "d0cf91c9-0193-49a2-b279-05eee1be2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SD3Transformer2DModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16,\n",
    ").to('cuda:1')\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-3-medium\",\n",
    "            subfolder=\"vae\",\n",
    "            revision=\"refs/pr/26\",\n",
    "    ).to('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57e0ff30-dd4d-45db-817b-f2bbcec03e65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "682ced49b6ff4b6ea50bb9965919a77a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "text_model1 = \"openai/clip-vit-large-patch14\"\n",
    "tokenizer1 = CLIPTokenizer.from_pretrained(text_model1)\n",
    "text_encoder1 = CLIPTextModelWithProjection.from_pretrained(text_model1)\n",
    "\n",
    "text_model2 = \"laion/CLIP-ViT-bigG-14-laion2B-39B-b160k\"\n",
    "tokenizer2 = CLIPTokenizer.from_pretrained(text_model2)\n",
    "text_encoder2 = CLIPTextModelWithProjection.from_pretrained(text_model2)\n",
    "\n",
    "text_model3 = \"google/t5-v1_1-xxl\"\n",
    "tokenizer3 = T5TokenizerFast.from_pretrained(text_model3)\n",
    "text_encoder3 = T5EncoderModel.from_pretrained(text_model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a29caa17-0f00-4b10-9d5d-a9ff9557c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_list = [dataset[0][\"txt\"], dataset[1][\"txt\"]]\n",
    "img_list = [dataset[0][\"jpg\"], dataset[1][\"jpg\"]]\n",
    "hint_list = [dataset[0][\"hint\"], dataset[1][\"hint\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b177234-1fcc-4c4d-89b9-adf7fb9dd6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_list = [text_encoder1, text_encoder2, text_encoder3]\n",
    "tokenizer_list = [tokenizer1, tokenizer2, tokenizer3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0c822d6e-24fc-4626-bb29-b0662369eb36",
   "metadata": {},
   "outputs": [],
   "source": [
    "for encoder in encoder_list:\n",
    "    for param in encoder.parameters():\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db0ed81c-c50e-4d3c-9a76-1025acc87871",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8846fdf-08fb-4505-aefc-f8ac563d0120",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_embeds1, pooled_prompt_embeds1 = encode_prompt(encoder_list, tokenizer_list, dataset[0]['txt'], 77)\n",
    "prompt_embeds2, pooled_prompt_embeds2 = encode_prompt(encoder_list, tokenizer_list, dataset[1]['txt'], 77)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4a51ce5-0dae-469d-b423-1458a1d95587",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e08bf76-d39d-40f5-b4f2-bb37ce7ceb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = [\n",
    "    {'prompt_embeds': prompt_embeds1, \n",
    "     'pooled_prompt_embeds': pooled_prompt_embeds1,\n",
    "     'prompt': prompt_list[0], \n",
    "     'img': img_list[0],\n",
    "     'hint': hint_list[0]\n",
    "    },\n",
    "    {'prompt_embeds': prompt_embeds2, \n",
    "     'pooled_prompt_embeds': pooled_prompt_embeds2,\n",
    "     'prompt': prompt_list[1], \n",
    "     'img': img_list[1],\n",
    "     'hint': hint_list[1]\n",
    "    }\n",
    "]\n",
    "torch.save(tensor_list, 'prompt_tensors_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56101bfc-f804-4475-aa24-dd1a9a914f38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tensor_list \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprompt_tensors_list.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "tensor_list = torch.load('prompt_tensors_list.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4764ae48-d68f-4c2f-aeaa-7210c36d4834",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers import FlowMatchEulerDiscreteScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35e19e72-6a60-4f6b-a7db-7c4303cd3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\", subfolder=\"scheduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "46d453e5-e0d0-418a-95bc-fc2bf441a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "994090ff-1ef7-4442-9c63-132e64cff882",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler_copy = copy.deepcopy(noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f572a2e0-d80a-493f-9ea5-705bd2843916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = torch.load('prompt_tensors_list.pt')\n",
    "for data in tensor_list:\n",
    "    for key in ['img', 'hint']:\n",
    "        data[key] = torch.tensor(data[key]).permute(2, 0, 1).unsqueeze(dim=0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0200ad8b-f171-4627-990e-2e9500b63f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = tensor_list[0]['img']\n",
    "hint_values = tensor_list[0]['img']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b5c011c1-2518-497e-b74e-2d643e517358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hint_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eb5e99bf-1c87-4c39-9047-a669fb7410b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pixel_values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "208970cc-458a-4776-8592-c7b24a1b895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = vae.encode(pixel_values).latent_dist.sample()\n",
    "model_input = (model_input - vae.config.shift_factor) * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d59b3207-191f-445b-aefd-d7ba9bb2482e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = model_input.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "fa37e646-413d-47b9-b127-0d5e79acd6a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0609"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.config.shift_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85e3d5be-6cde-4b28-8a73-53e1d3b40adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 09:05:44.982174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 09:05:44.982224: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 09:05:44.983174: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 09:05:44.989455: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 09:05:45.823735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from diffusers.training_utils import compute_density_for_timestep_sampling, compute_loss_weighting_for_sd3, free_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1a984e3-f4d6-4909-bc28-a441c661bc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1e682f03-17c2-4004-93ad-c6e24a22f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(model_input)\n",
    "bsz = model_input.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "# for weighting schemes where we sample timesteps non-uniformly\n",
    "u = compute_density_for_timestep_sampling(\n",
    "    weighting_scheme=\"logit_normal\",\n",
    "    batch_size=1,\n",
    "    logit_mean=0,\n",
    "    logit_std=1,\n",
    "    mode_scale=1.29,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b70c73d7-9923-45c2-bc2d-61bca4e4d9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = noise.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0d5e5514-7db2-437f-8f41-771f14320689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45001b51-6fae-4af2-ba66-e0322b6b1cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "750649f8-66bd-4308-8f8e-240e34f681f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "# Add noise according to flow matching.\n",
    "# zt = (1 - texp) * x + texp * z1\n",
    "sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype)\n",
    "noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1ce002ae-4c77-4296-8caa-709fbf7ac750",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64, 64])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80da458d-b07d-4ace-8e4d-f79f4a35f05f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          ...,\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843],\n",
       "          [0.9843, 0.9843, 0.9843,  ..., 0.9843, 0.9843, 0.9843]],\n",
       "\n",
       "         [[0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          ...,\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216],\n",
       "          [0.9216, 0.9216, 0.9216,  ..., 0.9216, 0.9216, 0.9216]],\n",
       "\n",
       "         [[0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          ...,\n",
       "          [0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039],\n",
       "          [0.8039, 0.8039, 0.8039,  ..., 0.8039, 0.8039, 0.8039]]]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hint_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4d394b96-b0b1-4c46-985d-29aee7d6e88d",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhint_values\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py:281\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[0;34m(self, x, return_dict)\u001b[0m\n\u001b[1;32m    279\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(encoded_slices)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(h)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py:255\u001b[0m, in \u001b[0;36mAutoencoderKL._encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_tiling \u001b[38;5;129;01mand\u001b[39;00m (width \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile_sample_min_size \u001b[38;5;129;01mor\u001b[39;00m height \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile_sample_min_size):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tiled_encode(x)\n\u001b[0;32m--> 255\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(enc)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/vae.py:143\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward method of the `Encoder` class.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_custom_forward\u001b[39m(module):\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cuda:1! (when checking argument for argument weight in method wrapper_CUDA__cudnn_convolution)"
     ]
    }
   ],
   "source": [
    "vae.encode(hint_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6979380d-3bad-480e-b926-67c0ba75d380",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0609"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.config.shift_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7988b000-e54c-447a-95b0-0e951944eff4",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m hint_input \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhint_values\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlatent_dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[1;32m      2\u001b[0m hint_input \u001b[38;5;241m=\u001b[39m (hint_input \u001b[38;5;241m-\u001b[39m vae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mshift_factor) \u001b[38;5;241m*\u001b[39m vae\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mscaling_factor\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/utils/accelerate_utils.py:46\u001b[0m, in \u001b[0;36mapply_forward_hook.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_hf_hook\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre_forward\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_hf_hook\u001b[38;5;241m.\u001b[39mpre_forward(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py:281\u001b[0m, in \u001b[0;36mAutoencoderKL.encode\u001b[0;34m(self, x, return_dict)\u001b[0m\n\u001b[1;32m    279\u001b[0m     h \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(encoded_slices)\n\u001b[1;32m    280\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 281\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    283\u001b[0m posterior \u001b[38;5;241m=\u001b[39m DiagonalGaussianDistribution(h)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/autoencoder_kl.py:255\u001b[0m, in \u001b[0;36mAutoencoderKL._encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_tiling \u001b[38;5;129;01mand\u001b[39;00m (width \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile_sample_min_size \u001b[38;5;129;01mor\u001b[39;00m height \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtile_sample_min_size):\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tiled_encode(x)\n\u001b[0;32m--> 255\u001b[0m enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    257\u001b[0m     enc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquant_conv(enc)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/autoencoders/vae.py:143\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, sample)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sample: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    141\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"The forward method of the `Encoder` class.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 143\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_in\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_checkpointing:\n\u001b[1;32m    147\u001b[0m         \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_custom_forward\u001b[39m(module):\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "hint_input = vae.encode(hint_values).latent_dist.sample()\n",
    "hint_input = (hint_input - vae.config.shift_factor) * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b939cbe4-8bbe-4ee2-8ae1-081a9de4c327",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_next_model = ControlNeXtModel().to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cf113f-a673-4ec1-96de-cbd776d5e3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "180fa436-a31d-4add-8424-d37d67c758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = control_next_model(hint_values, timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ffb1c9ae-ced4-4095-99f8-46d8c7ba6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2286f0de-3707-4b57-b855-1330710e8bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.resnet import Downsample2D, ResnetBlock2D\n",
    "    \n",
    "\n",
    "class ControlNeXtModel(ModelMixin, ConfigMixin):\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_embed_dim = 256,\n",
    "        in_channels = [128, 128],\n",
    "        out_channels = [128, 256],\n",
    "        groups = [4, 8],\n",
    "        controlnext_scale=1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_proj = Timesteps(128, True, downscale_freq_shift=0)\n",
    "        self.time_embedding = TimestepEmbedding(128, time_embed_dim)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.down_res = nn.ModuleList()\n",
    "        self.down_sample = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.down_res.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels[i],\n",
    "                    out_channels=out_channels[i],\n",
    "                    temb_channels=time_embed_dim,\n",
    "                    groups=groups[i]\n",
    "                ),\n",
    "            )\n",
    "            self.down_sample.append(\n",
    "                Downsample2D(\n",
    "                    out_channels[i],\n",
    "                    use_conv=True,\n",
    "                    out_channels=out_channels[i],\n",
    "                    padding=1,\n",
    "                    name=\"op\",\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.mid_convs = nn.ModuleList()\n",
    "        self.mid_convs.append(nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "        ))\n",
    "        self.mid_convs.append(\n",
    "            nn.Conv2d(\n",
    "            in_channels=out_channels[-1],\n",
    "            out_channels=320,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        ))\n",
    "\n",
    "        self.scale = controlnext_scale\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "    ):\n",
    "        \n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size = sample.shape[0]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        sample = self.embedding(sample)\n",
    "\n",
    "        for res, downsample in zip(self.down_res, self.down_sample):\n",
    "            sample = res(sample, emb)\n",
    "            sample = downsample(sample, emb)\n",
    "        \n",
    "        sample = self.mid_convs[0](sample) + sample\n",
    "        sample = self.mid_convs[1](sample)\n",
    "        \n",
    "        return {\n",
    "            'output': sample,\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e676cc70-ccdf-4218-bf68-d8702170e7e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4286a8c8-943e-4dc7-88bf-6500e7b75214",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32, device=\"cuda\"):\n",
    "    sigmas = noise_scheduler_copy.sigmas.to(device=device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler_copy.timesteps.to(device)\n",
    "    # timesteps = timesteps.to(accelerator.device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af065107-c10c-4acd-99ca-519012935d33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64, 64])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ed4e1d-ca8a-46cc-b982-eedbc1b48b0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5920])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2e304e6a-8499-4340-b52e-c7806594f081",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.5711,  0.6163,  0.5436,  ...,  0.5670,  0.5639,  0.7948],\n",
       "          [ 0.6272,  0.5906,  0.5291,  ...,  0.5508,  0.5644,  0.5648],\n",
       "          [ 0.6847,  0.5709,  0.5189,  ...,  0.5013,  0.4719,  0.5047],\n",
       "          ...,\n",
       "          [ 0.6466,  0.5639,  0.5026,  ...,  0.5060,  0.4690,  0.5258],\n",
       "          [ 0.7154,  0.6707,  0.5415,  ...,  0.5292,  0.5381,  0.6407],\n",
       "          [ 0.5723,  0.3418,  0.3065,  ...,  0.3226,  0.3396,  0.2554]],\n",
       "\n",
       "         [[-0.3030, -0.6567, -0.7042,  ..., -0.7500, -0.7010, -0.2314],\n",
       "          [-0.1118, -0.4534, -0.5227,  ..., -0.5604, -0.5018, -0.3567],\n",
       "          [-0.2342, -0.5176, -0.5859,  ..., -0.6250, -0.5460, -0.4082],\n",
       "          ...,\n",
       "          [-0.2341, -0.5109, -0.5464,  ..., -0.5850, -0.5115, -0.3681],\n",
       "          [-0.2872, -0.4887, -0.5481,  ..., -0.5756, -0.4824, -0.3423],\n",
       "          [-0.2067, -0.3374, -0.4073,  ..., -0.4105, -0.3632, -0.0780]],\n",
       "\n",
       "         [[ 0.7046,  0.7955,  0.7596,  ...,  0.7929,  0.8339,  1.0197],\n",
       "          [ 0.7299,  0.6098,  0.6304,  ...,  0.6861,  0.6155,  0.9749],\n",
       "          [ 0.6851,  0.6005,  0.6115,  ...,  0.6609,  0.5959,  0.9037],\n",
       "          ...,\n",
       "          [ 0.7184,  0.6180,  0.6221,  ...,  0.6567,  0.5872,  0.8973],\n",
       "          [ 0.6667,  0.5353,  0.5453,  ...,  0.5596,  0.5049,  0.8639],\n",
       "          [ 1.0193,  0.8930,  0.8420,  ...,  0.9030,  0.9174,  1.0843]],\n",
       "\n",
       "         ...,\n",
       "\n",
       "         [[-1.2053, -1.7741, -1.7430,  ..., -1.7330, -1.7298, -1.1629],\n",
       "          [-2.0946, -2.5424, -2.4673,  ..., -2.4847, -2.4298, -1.7909],\n",
       "          [-2.1388, -2.5968, -2.4998,  ..., -2.5222, -2.4840, -1.8237],\n",
       "          ...,\n",
       "          [-2.1583, -2.5420, -2.4895,  ..., -2.5052, -2.4411, -1.7922],\n",
       "          [-2.1310, -2.5985, -2.4970,  ..., -2.4929, -2.4178, -1.8224],\n",
       "          [-1.7243, -2.1840, -2.0960,  ..., -2.1241, -2.0328, -1.6609]],\n",
       "\n",
       "         [[-2.0564, -2.1501, -2.1554,  ..., -2.1697, -2.2552, -1.7649],\n",
       "          [-2.1946, -2.2138, -2.1557,  ..., -2.1988, -2.2371, -1.9125],\n",
       "          [-2.2014, -2.1927, -2.1379,  ..., -2.1477, -2.2093, -1.8835],\n",
       "          ...,\n",
       "          [-2.2195, -2.2186, -2.1377,  ..., -2.1448, -2.1988, -1.8795],\n",
       "          [-2.2704, -2.2320, -2.1914,  ..., -2.1975, -2.2624, -1.8500],\n",
       "          [-2.1098, -2.2706, -2.2333,  ..., -2.2541, -2.2942, -1.6037]],\n",
       "\n",
       "         [[-2.2644, -2.1512, -2.1509,  ..., -2.1521, -2.1345, -2.0259],\n",
       "          [-2.2968, -2.0648, -2.0541,  ..., -2.0491, -1.9960, -2.0359],\n",
       "          [-2.1702, -2.0065, -1.9855,  ..., -1.9770, -1.8899, -1.9414],\n",
       "          ...,\n",
       "          [-2.1350, -1.9397, -1.9233,  ..., -1.9349, -1.8736, -1.9354],\n",
       "          [-2.1526, -2.0152, -1.9571,  ..., -1.9777, -1.9352, -1.9717],\n",
       "          [-2.0658, -1.9574, -1.9197,  ..., -1.9153, -1.8793, -2.0792]]]],\n",
       "       grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5a8bce51-8786-48cd-af26-0f0caa8cbcdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64, 64])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_input.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
