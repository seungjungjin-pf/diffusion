{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56293b0f-0e94-4174-86f4-ede3d50aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84364b3f-9628-4881-a33d-cc29ab61f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 17:08:43.685652: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 17:08:43.685698: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 17:08:43.686846: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 17:08:43.692672: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 17:08:44.402631: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "\n",
    "\n",
    "from diffusers import SD3Transformer2DModel, AutoencoderKL, FlowMatchEulerDiscreteScheduler\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "\n",
    "from diffusers.training_utils import compute_density_for_timestep_sampling, compute_loss_weighting_for_sd3, free_memory\n",
    "\n",
    "\n",
    "from text_embed import encode_prompt, get_precomputed_tensors\n",
    "from datasets import FillDataset\n",
    "from sd3 import SD3CNModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bb275a-062c-4add-809f-50932a10a10f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.resnet import Downsample2D, ResnetBlock2D\n",
    "    \n",
    "\n",
    "class ControlNeXtModel(ModelMixin, ConfigMixin):\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_embed_dim = 256,\n",
    "        in_channels = [128, 128],\n",
    "        out_channels = [128, 256],\n",
    "        groups = [4, 8],\n",
    "        controlnext_scale=1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_proj = Timesteps(128, True, downscale_freq_shift=0)\n",
    "        self.time_embedding = TimestepEmbedding(128, time_embed_dim)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.down_res = nn.ModuleList()\n",
    "        self.down_sample = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.down_res.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels[i],\n",
    "                    out_channels=out_channels[i],\n",
    "                    temb_channels=time_embed_dim,\n",
    "                    groups=groups[i]\n",
    "                ),\n",
    "            )\n",
    "            self.down_sample.append(\n",
    "                Downsample2D(\n",
    "                    out_channels[i],\n",
    "                    use_conv=True,\n",
    "                    out_channels=out_channels[i],\n",
    "                    padding=1,\n",
    "                    name=\"op\",\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.mid_convs = nn.ModuleList()\n",
    "        self.mid_convs.append(nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "        ))\n",
    "        self.mid_convs.append(\n",
    "            nn.Conv2d(\n",
    "            in_channels=out_channels[-1],\n",
    "            out_channels=320,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        ))\n",
    "\n",
    "        self.scale = controlnext_scale\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "    ):\n",
    "        \n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size = sample.shape[0]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        sample = self.embedding(sample)\n",
    "\n",
    "        for res, downsample in zip(self.down_res, self.down_sample):\n",
    "            sample = res(sample, emb)\n",
    "            sample = downsample(sample, emb)\n",
    "        \n",
    "        sample = self.mid_convs[0](sample) + sample\n",
    "        sample = self.mid_convs[1](sample)\n",
    "        \n",
    "        return {\n",
    "            'output': sample,\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32, device=\"cuda\"):\n",
    "    sigmas = noise_scheduler_copy.sigmas.to(device=device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler_copy.timesteps.to(device)\n",
    "    # timesteps = timesteps.to(accelerator.device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2671a8c-3b3d-452b-bfaa-cd1b17e8095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "acbb6fcb-c61e-4358-a7ba-8a98db26b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer = SD3CNModel.from_pretrained(\n",
    "#     \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "#     subfolder=\"transformer\",\n",
    "#     torch_dtype=torch.float16).to(device)\n",
    "transformer = SD3CNModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0cf91c9-0193-49a2-b279-05eee1be2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_next_model = ControlNeXtModel().to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-3-medium\",\n",
    "            subfolder=\"vae\",\n",
    "            revision=\"refs/pr/26\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35e19e72-6a60-4f6b-a7db-7c4303cd3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\", subfolder=\"scheduler\"\n",
    ")\n",
    "noise_scheduler_copy = copy.deepcopy(noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f572a2e0-d80a-493f-9ea5-705bd2843916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = get_precomputed_tensors()\n",
    "for data in tensor_list:\n",
    "    for key, value in data.items():\n",
    "        if key != 'prompt' and not isinstance(value, torch.Tensor):\n",
    "            data[key] = torch.tensor(data[key]).to(device)\n",
    "        if key in ['img', 'hint']:\n",
    "            data[key] = data[key].permute(2, 0, 1).unsqueeze(dim=0)\n",
    "        \n",
    "        \n",
    "pixel_list = [x['img'] for x in tensor_list]\n",
    "hint_list = [x['hint'] for x in tensor_list]\n",
    "prompt_embed_list = [x['prompt_embeds'] for x in tensor_list]\n",
    "pooled_prompt_embed_list = [x['pooled_prompt_embeds'] for x in tensor_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208970cc-458a-4776-8592-c7b24a1b895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = vae.encode(pixel_list[0]).latent_dist.sample()\n",
    "model_input = (model_input - vae.config.shift_factor) * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b28eb48f-c4ec-4ab0-9a05-e0a706c6a375",
   "metadata": {},
   "outputs": [],
   "source": [
    "control_input = vae.encode(hint_list[0]).latent_dist.sample()\n",
    "control_input = (control_input - vae.config.shift_factor) * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e682f03-17c2-4004-93ad-c6e24a22f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(model_input)\n",
    "bsz = model_input.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "# for weighting schemes where we sample timesteps non-uniformly\n",
    "u = compute_density_for_timestep_sampling(\n",
    "    weighting_scheme=\"logit_normal\",\n",
    "    batch_size=1,\n",
    "    logit_mean=0,\n",
    "    logit_std=1,\n",
    "    mode_scale=1.29,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750649f8-66bd-4308-8f8e-240e34f681f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "# Add noise according to flow matching.\n",
    "# zt = (1 - texp) * x + texp * z1\n",
    "sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype, device=device)\n",
    "noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2bc7c5a2-9074-4128-aca2-954deeefd0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 64, 64])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "180fa436-a31d-4add-8424-d37d67c758cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = control_next_model(control_input, timesteps)\n",
    "res = control_next_model(hint_list[0], timesteps)\n",
    "control_out = res['output']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9ab07e3e-ca8d-452c-91b0-bba21f76fbd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "control_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "08520deb-fa32-4c9b-b25b-07751c61c458",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([851.], device='cuda:1', dtype=torch.float16)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "timesteps.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4e4999ba-0797-4627-8471-e411d3c49fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "print(noisy_model_input.dtype)\n",
    "print(timesteps.dtype)\n",
    "print(prompt_embeds.dtype)\n",
    "print(pooled_prompt_embeds.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f5772daf-078b-4e7e-80bd-dffd8cf0d63f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=1)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "38e63fe3-81e1-4488-a88f-230f52057bb5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m pooled_prompt_embeds \u001b[38;5;241m=\u001b[39m pooled_prompt_embed_list[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Predict the noise residual\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m model_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoisy_model_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimestep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_controlnet_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpooled_prompt_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/workspace/diffusion/sd3.py:158\u001b[0m, in \u001b[0;36mSD3CNModel.forward\u001b[0;34m(self, hidden_states, encoder_hidden_states, pooled_projections, timestep, block_controlnet_hidden_states, joint_attention_kwargs, return_dict)\u001b[0m\n\u001b[1;32m    155\u001b[0m height, width \u001b[38;5;241m=\u001b[39m hidden_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:]\n\u001b[1;32m    157\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embed(hidden_states)  \u001b[38;5;66;03m# takes care of adding positional embeddings too.\u001b[39;00m\n\u001b[0;32m--> 158\u001b[0m temb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtime_text_embed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimestep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpooled_projections\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    159\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_embedder(encoder_hidden_states)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index_block, block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks):\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/embeddings.py:1160\u001b[0m, in \u001b[0;36mCombinedTimestepTextProjEmbeddings.forward\u001b[0;34m(self, timestep, pooled_projection)\u001b[0m\n\u001b[1;32m   1157\u001b[0m timesteps_proj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtime_proj(timestep)\n\u001b[1;32m   1158\u001b[0m timesteps_emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep_embedder(timesteps_proj\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39mpooled_projection\u001b[38;5;241m.\u001b[39mdtype))  \u001b[38;5;66;03m# (N, D)\u001b[39;00m\n\u001b[0;32m-> 1160\u001b[0m pooled_projections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtext_embedder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_projection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m conditioning \u001b[38;5;241m=\u001b[39m timesteps_emb \u001b[38;5;241m+\u001b[39m pooled_projections\n\u001b[1;32m   1164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conditioning\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/embeddings.py:1661\u001b[0m, in \u001b[0;36mPixArtAlphaTextProjection.forward\u001b[0;34m(self, caption)\u001b[0m\n\u001b[1;32m   1660\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, caption):\n\u001b[0;32m-> 1661\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1662\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact_1(hidden_states)\n\u001b[1;32m   1663\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_2(hidden_states)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected all tensors to be on the same device, but found at least two devices, cuda:1 and cpu! (when checking argument for argument mat1 in method wrapper_CUDA_addmm)"
     ]
    }
   ],
   "source": [
    "# Get the text embedding for conditioning\n",
    "prompt_embeds = prompt_embed_list[0]\n",
    "pooled_prompt_embeds = pooled_prompt_embed_list[0]\n",
    "\n",
    "# Predict the noise residual\n",
    "model_pred = transformer(\n",
    "    hidden_states=noisy_model_input,\n",
    "    timestep=timesteps,\n",
    "    encoder_hidden_states=prompt_embeds,\n",
    "    pooled_projections=pooled_prompt_embeds,\n",
    "    block_controlnet_hidden_states=pooled_prompt_embeds,\n",
    "    return_dict=False,\n",
    ")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1c9ae-ced4-4095-99f8-46d8c7ba6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abcd86b8-5082-4189-85ae-97876e10166e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea514243-10d5-496c-a492-54a90c2fd4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
