{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56293b0f-0e94-4174-86f4-ede3d50aa420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84364b3f-9628-4881-a33d-cc29ab61f874",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-31 14:09:51.786066: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-31 14:09:51.786126: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-31 14:09:51.787179: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-31 14:09:51.793444: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-31 14:09:52.511625: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import logging\n",
    "\n",
    "\n",
    "from diffusers import SD3Transformer2DModel, AutoencoderKL, FlowMatchEulerDiscreteScheduler\n",
    "from transformers import CLIPTextModelWithProjection, CLIPTokenizer, T5EncoderModel, T5TokenizerFast\n",
    "\n",
    "from diffusers.training_utils import compute_density_for_timestep_sampling, compute_loss_weighting_for_sd3, free_memory\n",
    "\n",
    "\n",
    "from text_embed import encode_prompt, get_precomputed_tensors\n",
    "from datasets import FillDataset\n",
    "from sd3 import SD3ControlNextModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "273d9de6-4814-42d0-a780-ac2dcad7752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusers.configuration_utils import ConfigMixin, register_to_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2bb275a-062c-4add-809f-50932a10a10f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from diffusers.configuration_utils import ConfigMixin, register_to_config\n",
    "from diffusers.models.embeddings import TimestepEmbedding, Timesteps\n",
    "from diffusers.models.modeling_utils import ModelMixin\n",
    "from diffusers.models.resnet import Downsample2D, ResnetBlock2D\n",
    "    \n",
    "\n",
    "class ControlNeXtModel(ModelMixin, ConfigMixin):\n",
    "    _supports_gradient_checkpointing = True\n",
    "\n",
    "    @register_to_config\n",
    "    def __init__(\n",
    "        self,\n",
    "        time_embed_dim = 256,\n",
    "        in_channels = [128, 128],\n",
    "        out_channels = [128, 256],\n",
    "        groups = [4, 8],\n",
    "        controlnext_scale=1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.time_proj = Timesteps(128, True, downscale_freq_shift=0)\n",
    "        self.time_embedding = TimestepEmbedding(128, time_embed_dim)\n",
    "        self.embedding = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(2, 128),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.down_res = nn.ModuleList()\n",
    "        self.down_sample = nn.ModuleList()\n",
    "        for i in range(len(in_channels)):\n",
    "            self.down_res.append(\n",
    "                ResnetBlock2D(\n",
    "                    in_channels=in_channels[i],\n",
    "                    out_channels=out_channels[i],\n",
    "                    temb_channels=time_embed_dim,\n",
    "                    groups=groups[i]\n",
    "                ),\n",
    "            )\n",
    "            self.down_sample.append(\n",
    "                Downsample2D(\n",
    "                    out_channels[i],\n",
    "                    use_conv=True,\n",
    "                    out_channels=out_channels[i],\n",
    "                    padding=1,\n",
    "                    name=\"op\",\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.mid_convs = nn.ModuleList()\n",
    "        self.mid_convs.append(nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "            nn.Conv2d(\n",
    "                in_channels=out_channels[-1],\n",
    "                out_channels=out_channels[-1],\n",
    "                kernel_size=3,\n",
    "                stride=1,\n",
    "                padding=1\n",
    "            ),\n",
    "            nn.GroupNorm(8, out_channels[-1]),\n",
    "        ))\n",
    "        self.mid_convs.append(\n",
    "            nn.Conv2d(\n",
    "            in_channels=out_channels[-1],\n",
    "            out_channels=320,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "        ))\n",
    "\n",
    "        self.scale = controlnext_scale\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        sample: torch.FloatTensor,\n",
    "        timestep: Union[torch.Tensor, float, int],\n",
    "    ):\n",
    "        \n",
    "        timesteps = timestep\n",
    "        if not torch.is_tensor(timesteps):\n",
    "            # TODO: this requires sync between CPU and GPU. So try to pass timesteps as tensors if you can\n",
    "            # This would be a good case for the `match` statement (Python 3.10+)\n",
    "            is_mps = sample.device.type == \"mps\"\n",
    "            if isinstance(timestep, float):\n",
    "                dtype = torch.float32 if is_mps else torch.float64\n",
    "            else:\n",
    "                dtype = torch.int32 if is_mps else torch.int64\n",
    "            timesteps = torch.tensor([timesteps], dtype=dtype, device=sample.device)\n",
    "        elif len(timesteps.shape) == 0:\n",
    "            timesteps = timesteps[None].to(sample.device)\n",
    "\n",
    "        # broadcast to batch dimension in a way that's compatible with ONNX/Core ML\n",
    "        batch_size = sample.shape[0]\n",
    "        timesteps = timesteps.expand(batch_size)\n",
    "\n",
    "        t_emb = self.time_proj(timesteps)\n",
    "\n",
    "        # `Timesteps` does not contain any weights and will always return f32 tensors\n",
    "        # but time_embedding might actually be running in fp16. so we need to cast here.\n",
    "        # there might be better ways to encapsulate this.\n",
    "        t_emb = t_emb.to(dtype=sample.dtype)\n",
    "\n",
    "        emb = self.time_embedding(t_emb)\n",
    "\n",
    "        sample = self.embedding(sample)\n",
    "\n",
    "        for res, downsample in zip(self.down_res, self.down_sample):\n",
    "            sample = res(sample, emb)\n",
    "            sample = downsample(sample, emb)\n",
    "        \n",
    "        sample = self.mid_convs[0](sample) + sample\n",
    "        sample = self.mid_convs[1](sample)\n",
    "        \n",
    "        return {\n",
    "            'output': sample,\n",
    "            'scale': self.scale,\n",
    "        }\n",
    "\n",
    "\n",
    "def get_sigmas(timesteps, n_dim=4, dtype=torch.float32, device=\"cuda\"):\n",
    "    sigmas = noise_scheduler_copy.sigmas.to(device=device, dtype=dtype)\n",
    "    schedule_timesteps = noise_scheduler_copy.timesteps.to(device)\n",
    "    # timesteps = timesteps.to(accelerator.device)\n",
    "    step_indices = [(schedule_timesteps == t).nonzero().item() for t in timesteps]\n",
    "\n",
    "    sigma = sigmas[step_indices].flatten()\n",
    "    while len(sigma.shape) < n_dim:\n",
    "        sigma = sigma.unsqueeze(-1)\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2671a8c-3b3d-452b-bfaa-cd1b17e8095c",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b770392-0255-4528-822c-6717d2f60b9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load stabilityai/stable-diffusion-3-medium-diffusers  because context_embedder.bias expected shape tensor(..., device='meta', size=(1152,)), but got torch.Size([1536]). If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Load a pretrained model\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomSD3Transformer2DModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstabilityai/stable-diffusion-3-medium-diffusers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/modeling_utils.py:855\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because the following keys are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m missing: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Please make sure to pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m those weights or else make sure your checkpoint file is correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m     )\n\u001b[0;32m--> 855\u001b[0m unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_dict_into_meta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected:\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/model_loading_utils.py:223\u001b[0m, in \u001b[0;36mload_model_dict_into_meta\u001b[0;34m(model, state_dict, device, dtype, model_name_or_path, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quant_method_bnb:\n\u001b[1;32m    222\u001b[0m         model_name_or_path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mempty_state_dict[param_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_quantized \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_if_quantized_param(model, param, param_name, state_dict, param_device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    229\u001b[0m ):\n\u001b[1;32m    230\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, device, state_dict, unexpected_keys)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load stabilityai/stable-diffusion-3-medium-diffusers  because context_embedder.bias expected shape tensor(..., device='meta', size=(1152,)), but got torch.Size([1536]). If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example."
     ]
    }
   ],
   "source": [
    "from diffusers import SD3Transformer2DModel\n",
    "from huggingface_hub import PyTorchModelHubMixin\n",
    "\n",
    "\n",
    "class CustomSD3Transformer2DModel(SD3Transformer2DModel):\n",
    "    @register_to_config\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Initialize the parent class\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    # def from_pretrained(self, *args, **kwargs):\n",
    "    #     super().from_pretrained(*args, **kwargs)\n",
    "    \n",
    "    def forward(self, *args, **kwargs):\n",
    "        # Custom forward method implementation\n",
    "        # Add your custom behavior here\n",
    "        output = super().forward(*args, **kwargs)  # Call the original if needed\n",
    "        # Apply some modifications to the output if necessary\n",
    "        return output\n",
    "\n",
    "# Load a pretrained model\n",
    "custom_model = CustomSD3Transformer2DModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "acbb6fcb-c61e-4358-a7ba-8a98db26b041",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = SD3Transformer2DModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ba66d4-ed28-415e-9bb3-a35f5730113f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6cd6fc23-c6a5-4801-837f-287e3316f02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SD3Transformer2DModel.__init__() got an unexpected keyword argument 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# config = SD3Transformer2DModel.load_config(\"stabilityai/stable-diffusion-3-medium-diffusers\")\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Pass the configuration to your custom model\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m \u001b[43mCustomSD3Transformer2DModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Now load the weights\u001b[39;00m\n\u001b[1;32m      7\u001b[0m custom_model \u001b[38;5;241m=\u001b[39m CustomSD3Transformer2DModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-3-medium-diffusers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 8\u001b[0m, in \u001b[0;36mCustomSD3Transformer2DModel.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;66;03m# Initialize the parent class\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/configuration_utils.py:665\u001b[0m, in \u001b[0;36mregister_to_config.<locals>.inner_init\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    663\u001b[0m new_kwargs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_init_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs}\n\u001b[1;32m    664\u001b[0m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregister_to_config\u001b[39m\u001b[38;5;124m\"\u001b[39m)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mnew_kwargs)\n\u001b[0;32m--> 665\u001b[0m \u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: SD3Transformer2DModel.__init__() got an unexpected keyword argument 'config'"
     ]
    }
   ],
   "source": [
    "# config = SD3Transformer2DModel.load_config(\"stabilityai/stable-diffusion-3-medium-diffusers\")\n",
    "\n",
    "# Pass the configuration to your custom model\n",
    "custom_model = CustomSD3Transformer2DModel(config=config)\n",
    "\n",
    "# Now load the weights\n",
    "custom_model = CustomSD3Transformer2DModel.from_pretrained(\"stabilityai/stable-diffusion-3-medium-diffusers\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e25d40a4-2507-44e0-a32e-5b9cc49112ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot load stabilityai/stable-diffusion-3-medium-diffusers  because context_embedder.bias expected shape tensor(..., device='meta', size=(1152,)), but got torch.Size([1536]). If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m transformer2 \u001b[38;5;241m=\u001b[39m \u001b[43mSD3ControlNextModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstabilityai/stable-diffusion-3-medium-diffusers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/modeling_utils.py:855\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_keys) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    849\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because the following keys are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    850\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m missing: \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(missing_keys)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Please make sure to pass\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    851\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `low_cpu_mem_usage=False` and `device_map=None` if you want to randomly initialize\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    852\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m those weights or else make sure your checkpoint file is correct.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    853\u001b[0m     )\n\u001b[0;32m--> 855\u001b[0m unexpected_keys \u001b[38;5;241m=\u001b[39m \u001b[43mload_model_dict_into_meta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparam_device\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name_or_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_modules\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pat \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_keys_to_ignore_on_load_unexpected:\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/model_loading_utils.py:223\u001b[0m, in \u001b[0;36mload_model_dict_into_meta\u001b[0;34m(model, state_dict, device, dtype, model_name_or_path, hf_quantizer, keep_in_fp32_modules)\u001b[0m\n\u001b[1;32m    221\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_quant_method_bnb:\n\u001b[1;32m    222\u001b[0m         model_name_or_path_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 223\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    224\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot load \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name_or_path_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m expected shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mempty_state_dict[param_name]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    225\u001b[0m         )\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_quantized \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    228\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcheck_if_quantized_param(model, param, param_name, state_dict, param_device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    229\u001b[0m ):\n\u001b[1;32m    230\u001b[0m     hf_quantizer\u001b[38;5;241m.\u001b[39mcreate_quantized_param(model, param, param_name, device, state_dict, unexpected_keys)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot load stabilityai/stable-diffusion-3-medium-diffusers  because context_embedder.bias expected shape tensor(..., device='meta', size=(1152,)), but got torch.Size([1536]). If you want to instead overwrite randomly initialized weights, please make sure to pass both `low_cpu_mem_usage=False` and `ignore_mismatched_sizes=True`. For more information, see also: https://github.com/huggingface/diffusers/issues/1619#issuecomment-1345604389 as an example."
     ]
    }
   ],
   "source": [
    "transformer2 = SD3ControlNextModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0cf91c9-0193-49a2-b279-05eee1be2786",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "SD3ControlNextModel.__init__() missing 1 required positional argument: 'cnext_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m transformer \u001b[38;5;241m=\u001b[39m SD3Transformer2DModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-3-medium-diffusers\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransformer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      6\u001b[0m control_next_model \u001b[38;5;241m=\u001b[39m ControlNeXtModel()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 8\u001b[0m transformer2 \u001b[38;5;241m=\u001b[39m \u001b[43mSD3ControlNextModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontrol_next_model\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstabilityai/stable-diffusion-3-medium-diffusers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtransformer\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     13\u001b[0m vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstabilityai/stable-diffusion-3-medium\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m             subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m             revision\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrefs/pr/26\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/mambaforge/envs/py3.11/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/models/modeling_utils.py:821\u001b[0m, in \u001b[0;36mModelMixin.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m low_cpu_mem_usage:\n\u001b[1;32m    819\u001b[0m     \u001b[38;5;66;03m# Instantiate model with empty weights\u001b[39;00m\n\u001b[1;32m    820\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m accelerate\u001b[38;5;241m.\u001b[39minit_empty_weights():\n\u001b[0;32m--> 821\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munused_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    823\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    824\u001b[0m         hf_quantizer\u001b[38;5;241m.\u001b[39mpreprocess_model(\n\u001b[1;32m    825\u001b[0m             model\u001b[38;5;241m=\u001b[39mmodel, device_map\u001b[38;5;241m=\u001b[39mdevice_map, keep_in_fp32_modules\u001b[38;5;241m=\u001b[39mkeep_in_fp32_modules\n\u001b[1;32m    826\u001b[0m         )\n",
      "File \u001b[0;32m~/diffusers/src/diffusers/configuration_utils.py:260\u001b[0m, in \u001b[0;36mConfigMixin.from_config\u001b[0;34m(cls, config, return_unused_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m         init_dict[deprecated_kwarg] \u001b[38;5;241m=\u001b[39m unused_kwargs\u001b[38;5;241m.\u001b[39mpop(deprecated_kwarg)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Return model and optionally state and/or unused_kwargs\u001b[39;00m\n\u001b[0;32m--> 260\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minit_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;66;03m# make sure to also save config parameters that might be used for compatible classes\u001b[39;00m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# update _class_name\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_class_name\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m hidden_dict:\n",
      "\u001b[0;31mTypeError\u001b[0m: SD3ControlNextModel.__init__() missing 1 required positional argument: 'cnext_model'"
     ]
    }
   ],
   "source": [
    "transformer = SD3Transformer2DModel.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16).to(device)\n",
    "\n",
    "control_next_model = ControlNeXtModel().to(device)\n",
    "\n",
    "transformer2 = SD3ControlNextModel.from_pretrained(\n",
    "    control_next_model,\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\",\n",
    "    subfolder=\"transformer\",\n",
    "    torch_dtype=torch.float16).to(device)\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\n",
    "            \"stabilityai/stable-diffusion-3-medium\",\n",
    "            subfolder=\"vae\",\n",
    "            revision=\"refs/pr/26\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91bd7eba-baa3-4c3f-919e-648a192309fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer2 = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35e19e72-6a60-4f6b-a7db-7c4303cd3505",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise_scheduler = FlowMatchEulerDiscreteScheduler.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-3-medium-diffusers\", subfolder=\"scheduler\"\n",
    ")\n",
    "noise_scheduler_copy = copy.deepcopy(noise_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f572a2e0-d80a-493f-9ea5-705bd2843916",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_list = get_precomputed_tensors()\n",
    "for data in tensor_list:\n",
    "    for key, value in data.items():\n",
    "        if key != 'prompt' and not isinstance(value, torch.Tensor):\n",
    "            data[key] = torch.tensor(data[key]).to(device)\n",
    "        if key in ['img', 'hint']:\n",
    "            data[key] = data[key].permute(2, 0, 1).unsqueeze(dim=0)\n",
    "        \n",
    "        \n",
    "pixel_list = [x['img'] for x in tensor_list]\n",
    "hint_list = [x['hint'] for x in tensor_list]\n",
    "prompt_embed_list = [x['prompt_embeds'] for x in tensor_list]\n",
    "pooled_prompt_embed_list = [x['pooled_prompt_embeds'] for x in tensor_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "208970cc-458a-4776-8592-c7b24a1b895e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_input = vae.encode(pixel_list[0]).latent_dist.sample()\n",
    "model_input = (model_input - vae.config.shift_factor) * vae.config.scaling_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e682f03-17c2-4004-93ad-c6e24a22f1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample noise that we'll add to the latents\n",
    "noise = torch.randn_like(model_input)\n",
    "bsz = model_input.shape[0]\n",
    "# Sample a random timestep for each image\n",
    "# for weighting schemes where we sample timesteps non-uniformly\n",
    "u = compute_density_for_timestep_sampling(\n",
    "    weighting_scheme=\"logit_normal\",\n",
    "    batch_size=1,\n",
    "    logit_mean=0,\n",
    "    logit_std=1,\n",
    "    mode_scale=1.29,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "750649f8-66bd-4308-8f8e-240e34f681f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = (u * noise_scheduler_copy.config.num_train_timesteps).long()\n",
    "timesteps = noise_scheduler_copy.timesteps[indices].to(device=model_input.device)\n",
    "\n",
    "# Add noise according to flow matching.\n",
    "# zt = (1 - texp) * x + texp * z1\n",
    "sigmas = get_sigmas(timesteps, n_dim=model_input.ndim, dtype=model_input.dtype, device=device)\n",
    "noisy_model_input = (1.0 - sigmas) * model_input + sigmas * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b939cbe4-8bbe-4ee2-8ae1-081a9de4c327",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "180fa436-a31d-4add-8424-d37d67c758cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sj/diffusers/src/diffusers/models/downsampling.py:135: FutureWarning: `scale` is deprecated and will be removed in version 1.0.0. The `scale` argument is deprecated and will be ignored. Please remove it, as passing it will raise an error in the future. `scale` should directly be passed while calling the underlying pipeline component i.e., via `cross_attention_kwargs`.\n",
      "  deprecate(\"scale\", \"1.0.0\", deprecation_message)\n"
     ]
    }
   ],
   "source": [
    "res = control_next_model(hint_list[0], timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffb1c9ae-ced4-4095-99f8-46d8c7ba6541",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 320, 64, 64])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res['output'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea514243-10d5-496c-a492-54a90c2fd4b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
