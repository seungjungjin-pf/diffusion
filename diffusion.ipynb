{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bf07622-37d9-466f-afee-16366dcfc85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e748ff8-00fe-4c22-90bc-8314d6cacb7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils import data\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf571f3-1c93-466a-b52b-4ecf3ae7609e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import UNet, SinusoidalPositionEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccdaa11-d802-488a-b3a7-3a472853bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=False, transform=transform)\n",
    "\n",
    "class_idx = 3\n",
    "target_indices = [i for i, (_, label) in enumerate(trainset) if label == class_idx]\n",
    "\n",
    "# Create a Subset using these indices\n",
    "# filtered_dataset = Subset(trainset, target_indices[:64])\n",
    "filtered_dataset = Subset(trainset, target_indices)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(filtered_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=24)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf7830f-3b2c-47f1-8801-681d87014877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add noise\n",
    "def add_noise(x0, t, noise):\n",
    "    sqrt_alpha_cumprod = torch.sqrt(alphas_cumprod[t]).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    sqrt_one_minus_alpha_cumprod = torch.sqrt(1 - alphas_cumprod[t]).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "    return sqrt_alpha_cumprod * x0 + sqrt_one_minus_alpha_cumprod * noise\n",
    "\n",
    "# def generate_and_save_samples(epoch, model, device, writer):\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         # Start with a batch of random noise (generate 10 sample images for tracking)\n",
    "#         sample_noise = torch.randn(10, 3, 64, 64).to(device)  # Generate 10 sample images\n",
    "        \n",
    "#         # List to store images at regular intervals\n",
    "#         intermediate_images = []\n",
    "        \n",
    "#         # Gradually reverse the diffusion process\n",
    "#         for t in reversed(range(num_timesteps)):\n",
    "#             t_tensor = torch.tensor([t] * sample_noise.size(0), device=device).long()\n",
    "#             predicted_noise = model(sample_noise, t_tensor)\n",
    "#             alpha_t = torch.sqrt(alphas_cumprod[t])\n",
    "#             beta_t = torch.sqrt(1 - alphas_cumprod[t])\n",
    "#             sample_noise = (sample_noise - beta_t * predicted_noise) / alpha_t\n",
    "\n",
    "#             # Optional: Clip or scale the output at each step to avoid extreme values\n",
    "#             sample_noise = sample_noise.clamp(-1, 1)\n",
    "            \n",
    "#             # Save the images every 100 steps\n",
    "#             if t % 100 == 0 or t == 0:\n",
    "#                 img = (sample_noise + 1) / 2  # Convert [-1, 1] to [0, 1]\n",
    "#                 img = img.clamp(0, 1)\n",
    "#                 intermediate_images.append(img)\n",
    "\n",
    "#         # Stack the saved steps to form a tensor of shape: (num_intervals, 10, 3, 64, 64)\n",
    "#         # Each entry in the list is a batch of 10 images, so `torch.stack` creates this structure\n",
    "#         all_steps = torch.stack(intermediate_images, dim=0)  # Shape: (num_intervals, 10, 3, 64, 64)\n",
    "        \n",
    "#         # Rearrange to have a single batch where each row will represent the progression of one image\n",
    "#         all_steps = all_steps.permute(1, 0, 2, 3, 4)  # Shape: (10, num_intervals, 3, 64, 64)\n",
    "#         all_steps = all_steps.reshape(-1, 3, 64, 64)  # Shape: (10 * num_intervals, 3, 64, 64)\n",
    "\n",
    "#         # Create a grid where each row corresponds to the progression of one image across intervals\n",
    "#         grid = vutils.make_grid(all_steps, nrow=len(intermediate_images))\n",
    "#         writer.add_image(f'Progression at epoch {epoch + 1}', grid, epoch)\n",
    "\n",
    "\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "def generate_and_save_samples(epoch, model, device, writer=None):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Start with a batch of random noise (generate 10 sample images for tracking)\n",
    "        sample_noise = torch.randn(10, 3, 32, 32, device=device)  # CIFAR-10 images are 32x32 and RGB\n",
    "        \n",
    "        # List to store images at regular intervals\n",
    "        intermediate_images = []\n",
    "        \n",
    "        # Gradually reverse the diffusion process\n",
    "        for t in reversed(range(num_timesteps)):\n",
    "            t_tensor = torch.tensor([t] * sample_noise.size(0), device=device).long()\n",
    "            predicted_noise = model(sample_noise, t_tensor)\n",
    "            \n",
    "            # Scaling factors for the reverse step\n",
    "            alpha_t = torch.sqrt(alphas_cumprod[t])\n",
    "            beta_t = torch.sqrt(1 - alphas_cumprod[t])\n",
    "            \n",
    "            # Reverse step\n",
    "            sample_noise = (sample_noise - beta_t * predicted_noise) / alpha_t\n",
    "            \n",
    "            # Optional: Clip or scale the output at each step to avoid extreme values\n",
    "            sample_noise = sample_noise.clamp(-1, 1)\n",
    "            \n",
    "            # Save the images every 100 steps\n",
    "            if t % 100 == 0 or t == 0:\n",
    "                img = (sample_noise + 1) / 2  # Convert [-1, 1] to [0, 1]\n",
    "                img = img.clamp(0, 1)\n",
    "                intermediate_images.append(img)\n",
    "\n",
    "        # Stack and rearrange to create a grid of images showing the progression\n",
    "        all_steps = torch.stack(intermediate_images, dim=0)  # Shape: (num_intervals, 10, 3, 32, 32)\n",
    "        all_steps = all_steps.permute(1, 0, 2, 3, 4).reshape(-1, 3, 32, 32)  # Shape: (10 * num_intervals, 3, 32, 32)\n",
    "\n",
    "        # Create a grid where each row represents the progression of one image across intervals\n",
    "        grid = vutils.make_grid(all_steps, nrow=len(intermediate_images))\n",
    "        \n",
    "        # Save to TensorBoard\n",
    "        if writer:\n",
    "            writer.add_image(f'Progression at epoch {epoch + 1}', grid, epoch)\n",
    "        \n",
    "        # Optionally save directly to disk\n",
    "        vutils.save_image(grid, f\"generated_samples/progression_epoch_{epoch + 1}.png\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10e5a1-ae83-4670-82ba-bf3910f55662",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Assuming you have the modular UNet model, dataset, and noise scheduler\n",
    "model = UNet(in_channels=3, out_channels=3, time_emb_dim=256)  # Modular UNet\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Hyperparameters\n",
    "epochs = 5000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "num_timesteps = 1000\n",
    "\n",
    "# Define the beta schedule (linear schedule in this example)\n",
    "betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Loss function (Mean Squared Error for noise prediction)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# TensorBoard Writer\n",
    "import os\n",
    "\n",
    "base_log_dir = \"logs/ddpm_training\"\n",
    "run_number = 0\n",
    "\n",
    "# Increment run number until a new directory is found\n",
    "while os.path.exists(os.path.join(base_log_dir, f\"run_{run_number}\")):\n",
    "    run_number += 1\n",
    "\n",
    "log_dir = os.path.join(base_log_dir, f\"run_{run_number}\")\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "    running_loss = 0.0  # Track loss for the epoch\n",
    "\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Sample a random timestep for each image in the batch\n",
    "        t = torch.randint(0, num_timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample random noise\n",
    "        noise = torch.randn_like(images, device=device)\n",
    "        \n",
    "        # Get the noisy image for time step t\n",
    "        noisy_images = add_noise(images, t, noise)\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        predicted_noise = model(noisy_images, t)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = loss_fn(predicted_noise, noise)\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update the progress bar with the loss\n",
    "        running_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "        # break\n",
    "\n",
    "    # Log the average loss to TensorBoard\n",
    "    avg_loss = running_loss / len(trainloader)\n",
    "    writer.add_scalar(\"Loss/train\", avg_loss, epoch)\n",
    "\n",
    "    # Generate and log sample images to TensorBoard\n",
    "    if epoch % 10 == 0:\n",
    "        generate_and_save_samples(epoch, model, device, writer)\n",
    "\n",
    "    # Optional: Save model checkpoints\n",
    "    if epoch % 200 == 0:\n",
    "        torch.save(model.state_dict(), f\"weights/ddpm_{run_number}_epoch_{epoch + 1}.pth\")\n",
    "    # break\n",
    "\n",
    "# Close the TensorBoard writer\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d3ed6f-f522-4fef-8b46-665492c1c3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c577be0-9faf-474c-8161-998e9376050a",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "batch_size = 64\n",
    "learning_rate = 1e-4\n",
    "beta_start = 1e-4\n",
    "beta_end = 0.02\n",
    "num_timesteps = 1000\n",
    "\n",
    "# Define the beta schedule (linear schedule in this example)\n",
    "betas = torch.linspace(beta_start, beta_end, num_timesteps).to(device)\n",
    "alphas = 1.0 - betas\n",
    "alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    pbar = tqdm(trainloader, desc=f\"Epoch {epoch + 1}/{epochs}\")\n",
    "    for images, _ in pbar:\n",
    "        images = images.to(device)\n",
    "        batch_size = images.size(0)\n",
    "        \n",
    "        # Sample a random timestep for each image in the batch\n",
    "        t = torch.randint(0, num_timesteps, (batch_size,), device=device).long()\n",
    "        \n",
    "        # Sample random noise\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "        \n",
    "        # Get the noisy image for time step t\n",
    "        noisy_images = add_noise(images, t, noise)\n",
    "        break\n",
    "    break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec24dafe-4607-4c61-ac9a-365c19e0d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bcaaa7-076f-4eff-ade5-ae90b8009a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "noise[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f156bd-d178-49ab-a0ec-9939777843c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e6712b6-e695-45af-8bc7-1d2948d90521",
   "metadata": {},
   "outputs": [],
   "source": [
    "img1.shape, img2.shape, img3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c379a8c-e008-4d4c-8b2b-7b90d617f868",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734e4658-21b6-43d7-bbf2-ac4ddc3fb305",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for index in range(10):\n",
    "    img_list = []\n",
    "    img_list.append(images[index])\n",
    "    \n",
    "    for i in range(1000):\n",
    "        # Sample random noise\n",
    "        noise = torch.randn_like(images).to(device)\n",
    "        \n",
    "        # Get the noisy image for time step t\n",
    "        noisy_images = add_noise(images, i, noise)\n",
    "    \n",
    "        img_list.append(noisy_images[index])\n",
    "    combined = torch.stack(img_list)\n",
    "    \n",
    "    # Use make_grid to create a single frame\n",
    "    grid = vutils.make_grid(combined, nrow=25, padding=1)\n",
    "    plt.figure(figsize=(30,60))\n",
    "    plt.imshow(grid.permute(1, 2, 0).numpy())\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9153f347-280f-40dc-8b72-6f7a6b51f524",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from diffusers import UNet2DConditionModel\n",
    "\n",
    "# Load the UNet2DConditionModel\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\")\n",
    "\n",
    "# Print the structure of the UNet model to locate attention components\n",
    "print(unet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea885018-6cf3-4d9d-b922-bf727e46fa45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3.11",
   "language": "python",
   "name": "py3.11"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
